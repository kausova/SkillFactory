{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-07T14:28:59.144668Z","iopub.execute_input":"2021-10-07T14:28:59.145245Z","iopub.status.idle":"2021-10-07T14:28:59.155464Z","shell.execute_reply.started":"2021-10-07T14:28:59.145194Z","shell.execute_reply":"2021-10-07T14:28:59.154378Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport datetime\nimport pandas_profiling\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport pandas_profiling as pp\n%matplotlib inline\n\nfrom sklearn.feature_selection import f_classif, mutual_info_classif\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoost, CatBoostClassifier, Pool\nfrom catboost.utils import get_roc_curve\nfrom mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\nfrom math import log as log\nimport os","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:28:59.157175Z","iopub.execute_input":"2021-10-07T14:28:59.157537Z","iopub.status.idle":"2021-10-07T14:28:59.179246Z","shell.execute_reply.started":"2021-10-07T14:28:59.157496Z","shell.execute_reply":"2021-10-07T14:28:59.178176Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# обернем в класс все действия, связанные с предобработкой данных для лучшей воспроизводимости:\nclass Cust_preproc():\n    def __init__(self, data=None, column=None):\n        self.data = data\n        self.column = column\n    \n    def fill_na(self, data, column):\n        # функция заполнения пропусков модой\n        data[column].fillna(data[column].mode()[0], inplace=True)\n              \n    def show_outliers(self, data, column): \n        # рассчет выбросов методом IQR, вывод нижней и верхней границы; подсчет записей, которые не входят в диапазон\n        q25, q75 = data[column].quantile([0.25, 0.75])\n        IQR = q75 - q25\n        low_limit = q25 - 1.5 * IQR\n        up_limit = q75 + 1.5 * IQR\n        print(\"IQR range [{}, {}]\".format(low_limit, up_limit), \"\\nMin. value: {} \\nMax. value: {}\".\n          format(data[column].min(), data[column].max()))\n        print(\"Number of entries below the lower limit: {}, number of entries above the upper limit: {}\".\n        format(data[data[column] < low_limit][column].count(), data[data[column] > up_limit][column].count())) \n\ndef encode(data, option=\"w/dummy\"):\n# функция для преобразования признака Education в численный формат, кодирования бинарных переменных и создания dummy переменных из категориальных\n    if option == 'wo/dummy':\n    # опция без дамми переменных\n        keys = data['education'].unique()\n        values = range(1, data['education'].nunique()+1)\n        edc_labels = dict(zip(keys, values))\n        data['education'] = data['education'].map(edc_labels)\n        \n        label_encoder = LabelEncoder()   \n        for col in bin_cols:\n            data[col] = label_encoder.fit_transform(data[col])\n               \n    if option == 'w/dummy':\n    # опция с дамми переменными\n        keys = data['education'].unique()\n        values = range(1, data['education'].nunique()+1)\n        edc_labels = dict(zip(keys, values))\n        data['education'] = data['education'].map(edc_labels)\n        \n        label_encoder = LabelEncoder()   \n        for col in bin_cols:\n            data[col] = label_encoder.fit_transform(data[col])\n        \n        data = pd.get_dummies(data, columns=cat_cols)\n    return data\n        \ndef bin_quant(data, x):\n    # функция для разбивки значений по квартилям и превращения числового признака в категориальный\n    q1, mean, q3 = data[x].quantile([0.25, 0.5, 0.75])   \n    def split(y):\n        if y <= q1:\n            return 1\n        if q1 < y <= mean:\n            return 2\n        if mean < y <= q3:\n            return 3\n        if y > q3:\n            return 4       \n    data[\"bin_\"+x] = data[x].apply(lambda x: split(x)) # создание нового признака по квартилям\n      \ndef neat_matrix(cf_matrix):\n    # confusion matrix в подобающем виде \n    group_names = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"]\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')\n    plt.show()\n\ndef print_metrics(y_v, y_p, y_proba):\n    # рассчет f1 score, roc auc score и вывод confusion matrix определенной модели\n    f1_val = f1_score(y_v, y_p)\n    roc_auc = round(roc_auc_score(y_v, y_proba), 3)\n\n    cf_matrix1 = confusion_matrix(y_v, y_p)\n    \n    fpr, tpr, threshold = roc_curve(y_v, y_proba)\n    plt.figure()\n    plt.plot([0, 1], label='Baseline', linestyle='--')\n    plt.plot(fpr, tpr, label = 'Regression')\n    plt.title('Logistic Regression ROC AUC = %0.3f' % roc_auc)\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.legend(loc = 'lower right')\n    plt.show()\n\n    print('F1-score = {:.3f}'.format(f1_val))\n    print('ROC AUC = {:.3f}'.format(roc_auc))\n    neat_matrix(cf_matrix1)\n    \n    \ndef predict(data):\n    # реализация логистической регрессии\n    X = data[set(data.columns) - set(['default'])]\n    y = data['default']\n    \n    # деление на тренировочную и валидационную выборку с учетом дисбаланса классов\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=30, stratify=y)\n  \n    # стандартизация тренировочной выборки и валидационной на основе параметров тренировочной\n    scaler = StandardScaler().fit(X_train)\n    X_train_st = scaler.transform(X_train)\n    X_valid_st = scaler.transform(X_valid)\n\n    model = LogisticRegression(max_iter=10000)\n    model.fit(X_train_st, y_train)\n    y_pred = model.predict(X_valid_st)\n    y_pred_proba = model.predict_proba(X_valid_st)\n    y_pred_proba = y_pred_proba[:,1]\n    # оценка модели на основе f1 score, roc auc и confusion matrix\n    return print_metrics(y_valid, y_pred, y_pred_proba)\n\ndef predict_model(data, model):\n    # функция для оценки других моделей \n    X = data[set(data.columns) - set(['default'])]\n    y = data['default']\n    \n    # деление на тренировочную и валидационную выборку с учетом дисбаланса классов\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=30, stratify=y)\n  \n    # стандартизация тренировочной выборки и валидационной на основе параметров тренировочной\n    scaler = StandardScaler().fit(X_train)\n    X_train_st = scaler.transform(X_train)\n    X_valid_st = scaler.transform(X_valid)\n\n    # реализация модели на стандартных настройках и оценка на основе F1 и ROC AUC score\n    m = model\n    m.fit(X_train_st, y_train)\n    y_pred = m.predict(X_valid_st)\n    y_pred_proba = m.predict_proba(X_valid_st)\n    y_pred_proba = y_pred_proba[:,1]\n    f1 = f1_score(y_valid, y_pred)\n    roc = roc_auc_score(y_valid, y_pred_proba)\n    print(model, \"\\nF1_score: {:.3f}, ROC AUC score: {:.3f}\\n\".format(f1, roc))","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:28:59.180601Z","iopub.execute_input":"2021-10-07T14:28:59.181076Z","iopub.status.idle":"2021-10-07T14:28:59.215817Z","shell.execute_reply.started":"2021-10-07T14:28:59.181043Z","shell.execute_reply":"2021-10-07T14:28:59.214607Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# Information","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/sf-dst-scoring/'","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:28:59.238839Z","iopub.execute_input":"2021-10-07T14:28:59.239383Z","iopub.status.idle":"2021-10-07T14:28:59.243389Z","shell.execute_reply.started":"2021-10-07T14:28:59.239347Z","shell.execute_reply":"2021-10-07T14:28:59.242326Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(path +'/train.csv')\ntest = pd.read_csv(path +'test.csv')\nsample = pd.read_csv(path +'/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:28:59.245106Z","iopub.execute_input":"2021-10-07T14:28:59.245554Z","iopub.status.idle":"2021-10-07T14:28:59.522449Z","shell.execute_reply.started":"2021-10-07T14:28:59.245507Z","shell.execute_reply":"2021-10-07T14:28:59.521449Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"print(train.info())\nprint('Train size: ', train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:28:59.523692Z","iopub.execute_input":"2021-10-07T14:28:59.524219Z","iopub.status.idle":"2021-10-07T14:28:59.610026Z","shell.execute_reply.started":"2021-10-07T14:28:59.524169Z","shell.execute_reply":"2021-10-07T14:28:59.609096Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"print(test.info())\nprint('Test size: ', train.shape)\ntest.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:28:59.611435Z","iopub.execute_input":"2021-10-07T14:28:59.612115Z","iopub.status.idle":"2021-10-07T14:28:59.673125Z","shell.execute_reply.started":"2021-10-07T14:28:59.612052Z","shell.execute_reply":"2021-10-07T14:28:59.672178Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"print(sample.info())\nprint(sample.shape)\nsample.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:28:59.676821Z","iopub.execute_input":"2021-10-07T14:28:59.677210Z","iopub.status.idle":"2021-10-07T14:28:59.700720Z","shell.execute_reply.started":"2021-10-07T14:28:59.677174Z","shell.execute_reply":"2021-10-07T14:28:59.699417Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"train['train'] = 1 # помечаем тренировочные\ntest['train'] = 0 # помечаем тестовые\ndf = pd.concat([train, test], ignore_index=True)\ndf.info()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:28:59.704926Z","iopub.execute_input":"2021-10-07T14:28:59.705294Z","iopub.status.idle":"2021-10-07T14:28:59.831898Z","shell.execute_reply.started":"2021-10-07T14:28:59.705261Z","shell.execute_reply":"2021-10-07T14:28:59.830951Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"#   **EDA**","metadata":{}},{"cell_type":"markdown","source":"## *Data Description:*\nclient_id - identification\n\neducation - education level\n\nsex - sex\n\nage - age\n\ncar - binary/ has a car or not\n\ncar_type - whether car is international\n\ndecline_app_cnt - declined application count in the past\n\ngood_work - binary/ has 'good' work or not\n\nbki_request_cnt - requests to BKI\n\nhome_address - category of home address\n\nwork_address - category of work address\n\nincome - income\n\nforeign_passport - binary/ has foreign passport\n\nsna - connection with bank employee\n\nfirst_time - age of information about the client\n\nscore_bki - BKI score\n\nregion_rating - region rating\n\napp_date - application date\n\ndefault - default flag","metadata":{}},{"cell_type":"markdown","source":"**Посмотрим на переменные и расспределения, воспользовавшись уже готовым репортом из pandas profiling**","metadata":{}},{"cell_type":"code","source":"pp.ProfileReport(df)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:28:59.833513Z","iopub.execute_input":"2021-10-07T14:28:59.833935Z","iopub.status.idle":"2021-10-07T14:29:33.949054Z","shell.execute_reply.started":"2021-10-07T14:28:59.833888Z","shell.execute_reply":"2021-10-07T14:29:33.947866Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"В датасете всего 18 переменных (помимо переменной train, разделяющая тренировочную и тестовую выборку) и целевая переменная default\n\nИмеются признаки с различными типами данных - числовые, категориальные и бинарные. \n\nВ целевой переменной default в тренировочной выборке отсутствуют пропуски, зато имеется существенный дисбаланс классов.\n\nПропуски присутствуют лишь в одной переменной - Education. Их к-во не существенно.\n\nИз всех числовых переменных нормально распределена лишь score_bki. Все остальные имеют тяжелый левый хвост. \n\napp_date: данные представлены в виде даты. Это единственный признак, который должен быть значительно обработан для восприятия моделью. \n\nRegion_rating: хоть данные и предоставлены в числовом виде, имеется некоторая тенденция к категориальности, где значения распределены от 20 до 80 с шагом в 10. \n\nИcходя из контекста и описания, переменная car и car_type дублируют информацию друг друга. Возможно, эти две переменные будут иметь значительную коллинеарность (проверим позже)\n\nБинарные признаки будут обработаны с помощью LabelEncoder'a, а из категориальных признаков будут созданы dummy переменные.","metadata":{}},{"cell_type":"code","source":"# сгруппируем признаки по типам данных для последующего анализа\nnum_cols = ['age', 'decline_app_cnt', 'score_bki', 'income', 'bki_request_cnt', 'region_rating'] \ncat_cols = ['education', 'work_address', 'home_address', 'sna', 'first_time'] \nbin_cols = ['sex', 'car', 'car_type', 'good_work', 'foreign_passport'] ","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:29:33.950658Z","iopub.execute_input":"2021-10-07T14:29:33.951010Z","iopub.status.idle":"2021-10-07T14:29:33.956547Z","shell.execute_reply.started":"2021-10-07T14:29:33.950975Z","shell.execute_reply":"2021-10-07T14:29:33.955622Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# Наивная модель","metadata":{}},{"cell_type":"markdown","source":"Посмотрим самую базовую модель с минимальной обработкой датасета. \nДля этого необходимо лишь заполнить пропуски, признак app_date мы пока удалим, обработаем бинарные признаки и создать дамми-переменные для категориальных","metadata":{}},{"cell_type":"code","source":"# Заполняем пропуски, преобразовываем признаки для обработки моделью \npreproc = Cust_preproc()\npreproc.fill_na(df, 'education')\n\n# скопируем датасет перед переобразованием\ndf_t = df.drop('app_date', axis=1) # убираем пока что необработанный признак app_date\ndf_t = encode(df_t, option='wo/dummy') # кодируем бинарные переменные для восприятия моделью\n\ntrain = df_t.query('train == 1').drop(['train'], axis=1)\n\n# строим базовую модель без нормализации признаков\nX = train.drop('default', axis=1)\ny = train['default']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\ny_pred_proba = logreg.predict_proba(X_test)[:,1]\n\nprint_metrics(y_test, y_pred, y_pred_proba)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:29:33.957856Z","iopub.execute_input":"2021-10-07T14:29:33.958215Z","iopub.status.idle":"2021-10-07T14:29:35.073556Z","shell.execute_reply.started":"2021-10-07T14:29:33.958177Z","shell.execute_reply":"2021-10-07T14:29:35.072471Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"Очевидно, модель хорошо обучилась на преобладающем классе и определяет всех клиентов как не дефолтных, раздавая кредиты всем подряд. \nПродолжим дальше работать с датасетом, чтобы минимизировать процент на побочной диагонале матрицы (главным образом - FN) и улучшить ROC AUC","metadata":{}},{"cell_type":"markdown","source":"# Числовые признаки","metadata":{}},{"cell_type":"markdown","source":"Обработаем app_date в числовой признак: к-во дней с первой заявки в датасете и добавим его к числовым признакам для последующего анализа","metadata":{}},{"cell_type":"code","source":"# преобразуем в формат даты\ndf['app_date'] = pd.to_datetime(df['app_date'])\n\n# вычислим к-во дней с самой давней записи в датасете \ndf['app_days'] = df['app_date'].apply(lambda x: x - df['app_date'].min())\ndf['app_days'] = df['app_days'].dt.days\ndf.drop('app_date', axis=1, inplace=True)\n\n# обновим список с числовыми признаками\nnum_cols = num_cols + ['app_days']","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:29:35.075462Z","iopub.execute_input":"2021-10-07T14:29:35.075877Z","iopub.status.idle":"2021-10-07T14:30:22.905156Z","shell.execute_reply.started":"2021-10-07T14:29:35.075837Z","shell.execute_reply":"2021-10-07T14:30:22.903712Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# расспределения дефолтных клиентов относительно числовых признаков (без app_days пока)\nfig, axes = plt.subplots(2, 3, figsize=(25,15))\nfor i, col in enumerate(num_cols[:-1]):\n    sns.boxplot(y = df[col], x = 'default', data=df, ax=axes.flat[i])\n    print(round(df.groupby('default')[col].mean(),2))","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:22.906646Z","iopub.execute_input":"2021-10-07T14:30:22.906997Z","iopub.status.idle":"2021-10-07T14:30:24.194127Z","shell.execute_reply.started":"2021-10-07T14:30:22.906952Z","shell.execute_reply":"2021-10-07T14:30:24.192668Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"1. Дефолтные клиенты в среднем моложе\n2. БОльшее к-во отказанных заявок в прошлом\n3. БОльший показатель BKI score\n4. Меньший доход\n5. БОльшее к-во запросов \n6. Меньшее значение по показателю region rating\n","metadata":{}},{"cell_type":"code","source":"# отдельно еще раз сравним расспределения числовых переменных\nfig, axes = plt.subplots(2, 4, figsize=(25,15))\nfor i, col in enumerate(num_cols):\n    sns.histplot(df[col], ax=axes.flat[i])","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:24.195683Z","iopub.execute_input":"2021-10-07T14:30:24.196142Z","iopub.status.idle":"2021-10-07T14:30:29.872359Z","shell.execute_reply.started":"2021-10-07T14:30:24.196094Z","shell.execute_reply":"2021-10-07T14:30:29.871238Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"Расспределения почти всех числовых переменных имеют тяжелый правый хвост. Примечательно, что score_bki расспределен нормально","metadata":{}},{"cell_type":"code","source":"# значимость числовых переменных\ntrain = df.query('train == 1').drop(['train'], axis=1)\nimp_num = pd.Series(f_classif(train[num_cols], train['default'])[0], index = num_cols)\nimp_num.sort_values(inplace = True)\nimp_num.plot(kind = 'barh')","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:29.874349Z","iopub.execute_input":"2021-10-07T14:30:29.874857Z","iopub.status.idle":"2021-10-07T14:30:30.103631Z","shell.execute_reply.started":"2021-10-07T14:30:29.874810Z","shell.execute_reply":"2021-10-07T14:30:30.102236Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Категориальные и бинарные признаки","metadata":{}},{"cell_type":"markdown","source":"Смотрим на % распределение дефолтных и недефолтных клиентов по категориальным и бинарным признакам","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 5, figsize=(25, 15))\nfor i, col in enumerate(cat_cols + bin_cols):\n    new_df = df.groupby(col)['default'].value_counts(normalize=True)\n    new_df = new_df.mul(100).rename('% - percent').reset_index()\n    sns.barplot(x=col, y='% - percent', hue='default', data=new_df, ax=axes.flat[i])","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:30.105328Z","iopub.execute_input":"2021-10-07T14:30:30.105907Z","iopub.status.idle":"2021-10-07T14:30:32.016273Z","shell.execute_reply.started":"2021-10-07T14:30:30.105854Z","shell.execute_reply":"2021-10-07T14:30:32.015025Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"Интересно, что самый большой процент дефолтных клиентов:\n- С уровнем образования SCH и UGR\n- С рабочим адресом категории 3\n- С домашним адресом категории 2\n- Почти оданаковое процентное расспределение по sna - зеркально распределению по признаку first_time. Достаточно логично, ведь чем дольше история клиента в банке (ближе к 4) - тем лучше его отношения с банковским работником (ближе к 1)\n- В категории 1 по признаку first_time, несмотря на то, что эта категория в датасете далеко не самая многочисленная\n\nДанные наблюдения могут быть воспользованы для уменьшении размерности матрицы признаков через объединение некоторых категорий в одну.\n\n- Относительно пола, процент дефолтных несколько выше у мужчин\n- Больше дефолтных клиентов не имеют машину, либо имеют, но не иномарку. У них нету загранпаспорта и хуже работа.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"Посмотрим на значимость категориальный и бинарных переменных методом ANOVA","metadata":{}},{"cell_type":"code","source":"# значимость категориальных и бинарных переменных\ntrain = df.query('train == 1').drop(['train'], axis=1)\n# преобразуем бинарные признаки и категориальные в числовые\ntrain = encode(train, option='wo/dummy')\n\nimp_cat = pd.Series(mutual_info_classif(train[bin_cols + cat_cols], train['default'],\ndiscrete_features=True), index=bin_cols + cat_cols)\nimp_cat.sort_values(inplace=True)\nimp_cat.plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:32.018432Z","iopub.execute_input":"2021-10-07T14:30:32.018819Z","iopub.status.idle":"2021-10-07T14:30:32.570564Z","shell.execute_reply.started":"2021-10-07T14:30:32.018785Z","shell.execute_reply":"2021-10-07T14:30:32.569615Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"# Корреляции","metadata":{}},{"cell_type":"code","source":"sns.heatmap(train.corr())","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:32.571586Z","iopub.execute_input":"2021-10-07T14:30:32.571853Z","iopub.status.idle":"2021-10-07T14:30:33.199005Z","shell.execute_reply.started":"2021-10-07T14:30:32.571826Z","shell.execute_reply":"2021-10-07T14:30:33.197510Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"corr = train.corr().abs()\ntrain_corr = corr.unstack()\ntrain_corr = train_corr.sort_values(ascending=False).drop_duplicates()\ntrain_corr[0:11]","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:33.200915Z","iopub.execute_input":"2021-10-07T14:30:33.201246Z","iopub.status.idle":"2021-10-07T14:30:33.293836Z","shell.execute_reply.started":"2021-10-07T14:30:33.201215Z","shell.execute_reply":"2021-10-07T14:30:33.292563Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"Сильная корреляция между id клиента и app_days - скорей всего, id генерировались по возрастанию с первой записи в системе. Так же имеется существенная корреляция между home и work address (люди обычно выбирают жилье недалеко от работы либо наоборот). Как было замечено ранее, car и car_type дублируют информацию в некотором роде - поэтому переменные скоррелированы. Из матрицы корреляций, видно, что sna и first_time негативно скоррелированы. Наличие иномарки не сильно, но коррелирует с уровнем дохода.\n","metadata":{}},{"cell_type":"markdown","source":"# Выбросы","metadata":{}},{"cell_type":"code","source":"# смотрим на выбросы числовых признаков\nfor col in num_cols:\n    print(col)\n    preproc.show_outliers(df, col)\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:33.295241Z","iopub.execute_input":"2021-10-07T14:30:33.295669Z","iopub.status.idle":"2021-10-07T14:30:33.366555Z","shell.execute_reply.started":"2021-10-07T14:30:33.295621Z","shell.execute_reply":"2021-10-07T14:30:33.365438Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"Выбросов в признаке Age не найдено\nDecline_app_cnt: в данной переменной 83% значений нулевые. Убирать все значения выше 0 - абсолютно лишено смысла. Посмотрим на процентное распределение уникальных значений","metadata":{}},{"cell_type":"code","source":"df['decline_app_cnt'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:33.367796Z","iopub.execute_input":"2021-10-07T14:30:33.368093Z","iopub.status.idle":"2021-10-07T14:30:33.378318Z","shell.execute_reply.started":"2021-10-07T14:30:33.368063Z","shell.execute_reply":"2021-10-07T14:30:33.377049Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"Значения выше 4 встречаются у меньше 0.5% данных. Предлагаю заменить все значения выше 4 - 4 и использовать как категориальный признак при построении модели ","metadata":{}},{"cell_type":"code","source":"df.loc[df['decline_app_cnt'] > 4, 'decline_app_cnt'] = 4","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:33.379563Z","iopub.execute_input":"2021-10-07T14:30:33.379873Z","iopub.status.idle":"2021-10-07T14:30:33.388685Z","shell.execute_reply.started":"2021-10-07T14:30:33.379843Z","shell.execute_reply":"2021-10-07T14:30:33.387479Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"Похожая ситуация с переменной bki_request_cnt. Проделаем то же самое. ","metadata":{}},{"cell_type":"code","source":"df['bki_request_cnt'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:33.390027Z","iopub.execute_input":"2021-10-07T14:30:33.390623Z","iopub.status.idle":"2021-10-07T14:30:33.405684Z","shell.execute_reply.started":"2021-10-07T14:30:33.390574Z","shell.execute_reply":"2021-10-07T14:30:33.404654Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# определим порог значением 5. \ndf.loc[df['bki_request_cnt'] > 5, 'bki_request_cnt'] = 5","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:33.407046Z","iopub.execute_input":"2021-10-07T14:30:33.407685Z","iopub.status.idle":"2021-10-07T14:30:33.414640Z","shell.execute_reply.started":"2021-10-07T14:30:33.407639Z","shell.execute_reply":"2021-10-07T14:30:33.413405Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"В случае income, порог в 90000 так же не сильно реалистичен. Исходя из графиков распределения (выше), условная граница, после которой почти нету значений - 0.4 (на графике). Выберем верхнюю границу в 500к и заменим этим значением все, что выше.","metadata":{}},{"cell_type":"code","source":"df.loc[df['income'] > 500000, 'income'] = 500000","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:33.416485Z","iopub.execute_input":"2021-10-07T14:30:33.416878Z","iopub.status.idle":"2021-10-07T14:30:33.427404Z","shell.execute_reply.started":"2021-10-07T14:30:33.416843Z","shell.execute_reply":"2021-10-07T14:30:33.426195Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"Score_bki: установим нижнюю границу согласно подсчетам. Для верхней границы используем значение 0 (таким образом модель показывает наилучший результат)","metadata":{}},{"cell_type":"code","source":"df.loc[df['score_bki'] < -3.299, 'score_bki'] = -3.299\ndf.loc[df['score_bki'] > 0, 'score_bki'] = 0","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:33.429179Z","iopub.execute_input":"2021-10-07T14:30:33.429550Z","iopub.status.idle":"2021-10-07T14:30:33.443953Z","shell.execute_reply.started":"2021-10-07T14:30:33.429515Z","shell.execute_reply":"2021-10-07T14:30:33.442570Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"# Feauture Engineering","metadata":{}},{"cell_type":"code","source":"# создание нового признака - среднего значения дохода по рейтингу (используем только тренировочные данные)\nmean_region_income = train.groupby('region_rating')['income'].mean().to_dict()\ndf['mean_region_income'] = df['region_rating'].map(mean_region_income)\n\n# создание нового признака - среднего значения score_bki по рейтингу (используем только тренировочные данные)\nmean_region_bki = train.groupby('region_rating')['score_bki'].mean().to_dict()\ndf['mean_region_bki'] = df['region_rating'].map(mean_region_bki)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:33.445574Z","iopub.execute_input":"2021-10-07T14:30:33.445881Z","iopub.status.idle":"2021-10-07T14:30:33.469469Z","shell.execute_reply.started":"2021-10-07T14:30:33.445851Z","shell.execute_reply":"2021-10-07T14:30:33.468119Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# преобразуем бинарные переменные в числовые\ndf = encode(df,option=\"wo/dummy\" )","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:33.470982Z","iopub.execute_input":"2021-10-07T14:30:33.471456Z","iopub.status.idle":"2021-10-07T14:30:33.694477Z","shell.execute_reply.started":"2021-10-07T14:30:33.471419Z","shell.execute_reply":"2021-10-07T14:30:33.693150Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# объеденим два признака в один, определяющий категорию: 0 - машины нету, 1 - есть, но не иномарка, 2 - есть, иномарка.\ndf['car_cat'] = df['car'] + df['car_type']\ndf.drop(['car', 'car_type'], axis=1, inplace=True)\n\n# на основе таких числовые признаков, как age и income, создадим категориальные, разбив на категории-квантили\nbin_quant(df,'age')\nbin_quant(df,'income')\n\ndf.drop(['age', 'income'], axis=1, inplace=True)\n\n# разобъем значения region_rating на две категории: выше и ниже 50\ndf['region_rating'] = df['region_rating'].apply(lambda x: 1 if x >= 50 else 0) \n\n# создадим полином третьей и 5той степени для score_bki, как для признака с наибольшей значимостью\ndf['score_bki_3'] = df['score_bki'].apply(lambda x: x ** 3)\n#df['score_bki_5'] = df['score_bki'].apply(lambda x: x ** 5)\n\n# создадим категориальный признак на основе bki_score (1 если выше среднего и 0 - если ниже)\ndf['bki_class'] = df['score_bki'].apply(lambda x: 1 if x >= train['score_bki'].mean() else 0)\n\n# app_days сильно скоррелирован с client_id. переделаем его в категориальный признак, чтобы оставить некоторую информацию\ndf['app_days_class'] = df['app_days'].apply(lambda x: 1 if x >= train['app_days'].mean() else 0)\ndf.drop('app_days', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:30:33.695883Z","iopub.execute_input":"2021-10-07T14:30:33.696241Z","iopub.status.idle":"2021-10-07T14:31:09.964343Z","shell.execute_reply.started":"2021-10-07T14:30:33.696208Z","shell.execute_reply":"2021-10-07T14:31:09.963283Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"Проведем декомпозицую адресов (work_address + home_address), объеденив два стандартизированных вектора в один с помощью PCA","metadata":{}},{"cell_type":"code","source":"#Декомпозиция адресов\n\n# создадим Scaler\nscaler = StandardScaler()\n# здесь осторожно: нельзя вычислять среднее и отклонение по всей выборке, так как сольем данные по тестовой выборке, чего делать нельзя\n\n# fit+transform применим к обучающей выборке, а к тестовой - только transform\n\ndata_addresses_train = df[df['train'] == 1][['work_address', 'home_address']].values\ndata_addresses_test = df[df['train'] == 0][['work_address', 'home_address']].values\nscaled_data_train = scaler.fit_transform(data_addresses_train)\nscaled_data_test = scaler.transform(data_addresses_test)\nscaled_data = np.concatenate((scaled_data_train, scaled_data_test))\n\n#У нас два вектора. Сократим до одного, оставив наиболее значимую информацию.\npca = PCA(n_components=1)\n# в основе PCA лежит центрирование. похожая история, что и ранее. fit по тренировочной выборке, а transform на всей\npca.fit(scaled_data_train)\npca_data_train = pca.transform(scaled_data_train)\npca_data_test = pca.transform(scaled_data_test)\ndf['pca_address'] = np.concatenate((pca_data_train, pca_data_test))\ndf.drop(['home_address', 'work_address'], axis=1, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:31:09.965771Z","iopub.execute_input":"2021-10-07T14:31:09.966092Z","iopub.status.idle":"2021-10-07T14:31:10.038397Z","shell.execute_reply.started":"2021-10-07T14:31:09.966062Z","shell.execute_reply":"2021-10-07T14:31:10.037065Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"Несмотря на то, что pca и sna сильную отрицательную корреляцию, удаление хотя бы одного из них ухудшают качество модели. Попробуем создать новый признак на основе двух, воспользовавшись тем же приемом, что и с адресами. ","metadata":{}},{"cell_type":"code","source":"# Декомпозиция pca и sna\n\n# разделяем тестовый и обучающий датасет для корректной стандартизации\ndata_train = df[df['train'] == 1][['sna', 'first_time']].values\ndata_test = df[df['train'] == 0][['sna', 'first_time']].values\n\nscaled_data_train = scaler.fit_transform(data_train)\nscaled_data_test = scaler.transform(data_test)\nscaled_data = np.concatenate((scaled_data_train, scaled_data_test))\n\n# сокращаем до одного вектора, центрируя выборки отдельно друг от друга\npca.fit(scaled_data_train)\npca_data_train = pca.transform(scaled_data_train)\npca_data_test = pca.transform(scaled_data_test)\ndf['pca_sna_first_time'] = np.concatenate((pca_data_train, pca_data_test))\ndf.drop(['sna', 'first_time'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:31:10.040294Z","iopub.execute_input":"2021-10-07T14:31:10.041026Z","iopub.status.idle":"2021-10-07T14:31:10.132254Z","shell.execute_reply.started":"2021-10-07T14:31:10.040978Z","shell.execute_reply":"2021-10-07T14:31:10.130485Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# обновляем списки переменных\nnum_cols = ['score_bki', 'mean_region_income', 'mean_region_bki', 'score_bki_3'] \ncat_cols = ['education', 'app_days_class', 'bki_class', 'car_cat', 'pca_address', 'pca_sna_first_time', 'decline_app_cnt', 'bki_request_cnt', 'bin_age', 'bin_income'] \nbin_cols = ['foreign_passport', 'region_rating', 'good_work', 'sex'] ","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:31:10.134384Z","iopub.execute_input":"2021-10-07T14:31:10.135129Z","iopub.status.idle":"2021-10-07T14:31:10.141546Z","shell.execute_reply.started":"2021-10-07T14:31:10.135079Z","shell.execute_reply":"2021-10-07T14:31:10.140395Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"Смотрим, насколько улучшились показатели после проведенных действий","metadata":{}},{"cell_type":"code","source":"# создаем дамми переменные для категориальных признаков\ndf = pd.get_dummies(df, columns=cat_cols)\n\ntrain = df.query('train == 1').drop(['train'], axis=1)\npredict(train)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:31:10.143387Z","iopub.execute_input":"2021-10-07T14:31:10.144079Z","iopub.status.idle":"2021-10-07T14:31:11.851204Z","shell.execute_reply.started":"2021-10-07T14:31:10.144033Z","shell.execute_reply":"2021-10-07T14:31:11.850275Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"ROC AUC score теперь выглядит намного лучше: c помощью предобработки, а так же стандартизации данных удалось увеличить метрику с 0.60 до 0.739 и немного поднять F1. Модель теперь смогла правильно определить некоторое число дефолтных клиентов.","metadata":{}},{"cell_type":"markdown","source":"Посмотрим на значимость признаков в обновленном обработанном датасете и удалим те, у которых показатель равен 0","metadata":{}},{"cell_type":"code","source":"df_temp = df.query('train == 1').drop('train', axis=1)\nnum_columns = ['client_id', 'score_bki', 'mean_region_income', 'mean_region_bki']\n# обновляем список бинарных и категориальных признаков, значимость которых будет оцениваться\nbin_cat_columns = set(df_temp.columns) - set(num_columns) - set(['default'])\n\nimp_cat = pd.Series(mutual_info_classif(df_temp[bin_cat_columns], df_temp['default'],\ndiscrete_features=False), index=bin_cat_columns)\n\n# создаем список из наименее значимых признаков для их удаления\ncat_cols_to_drop = list(imp_cat[imp_cat==0].index)\ndf.drop(cat_cols_to_drop, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:31:11.857103Z","iopub.execute_input":"2021-10-07T14:31:11.857502Z","iopub.status.idle":"2021-10-07T14:31:49.328703Z","shell.execute_reply.started":"2021-10-07T14:31:11.857461Z","shell.execute_reply":"2021-10-07T14:31:49.327799Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# то же самое для численных переменных - практика показала, что их удалять не стоит\n#corr = df.corr().abs()\n#train_corr = corr.unstack()\n#train_corr = train_corr.sort_values(ascending=False).drop_duplicates()\n#corr_to_drop = train_corr[1:][train_corr>0.7].reset_index()['level_1'].to_list()\n#df.drop(corr_to_drop, axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:31:49.330171Z","iopub.execute_input":"2021-10-07T14:31:49.330671Z","iopub.status.idle":"2021-10-07T14:31:49.334165Z","shell.execute_reply.started":"2021-10-07T14:31:49.330620Z","shell.execute_reply":"2021-10-07T14:31:49.333279Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"# Модели","metadata":{}},{"cell_type":"markdown","source":"Посмотрим, как справятся другие модели на стандартных настройках","metadata":{}},{"cell_type":"code","source":"# обновим тренировочную выборку\ntrain = df.query('train == 1').drop(['train'], axis=1)\n\nlogreg = LogisticRegression(max_iter=1000)\nrforest = RandomForestClassifier(n_estimators=1000)\nGB = GradientBoostingClassifier(n_estimators=1000)\nad = AdaBoostClassifier(n_estimators=1000)\n\nmodels = [logreg, rforest, GB, ad]\n\nfor model in models:\n    predict_model(train, model)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:31:49.335381Z","iopub.execute_input":"2021-10-07T14:31:49.335845Z","iopub.status.idle":"2021-10-07T14:37:38.432318Z","shell.execute_reply.started":"2021-10-07T14:31:49.335805Z","shell.execute_reply":"2021-10-07T14:37:38.431066Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"Лучший показатель ROC AUC score у логистической регрессии, но ее точность оставляет желать лучшего.\n\nДостаточно обещающие результаты у GradientBoostingClassifier в контексте F1 score. ROC AUC при этом несколько хуже. \n\nAdaBoostClassifier - некоторый trade-off между F1 score и ROC AUC. \n\nПродолжим работать с логистической регрессией дальше, оставив другие модели на потом","metadata":{}},{"cell_type":"markdown","source":"## **Подбор гиперпараметров**","metadata":{}},{"cell_type":"markdown","source":"попробуем поискать лучшую комбинацию параметров для Logistic Regression с помощью GridSearch (!грузится столетие!)","metadata":{}},{"cell_type":"code","source":"X = train.drop('default', axis=1)\ny = train['default']\n\n# определяем параметры\nsolvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag']\npenalty = ['none', 'l1', 'l2']\nc_values = [100, 10, 1.0, 0.1]\n\nmodel = LogisticRegression()\n \n# определяем grid search и запускаем поиск лучшей комбинации\ngrid = dict(solver=solvers, penalty=penalty, C=c_values)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=3, scoring='roc_auc',error_score=0)\ngrid_result = grid_search.fit(X, y)\n\n# выводим результаты\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:37:38.433988Z","iopub.execute_input":"2021-10-07T14:37:38.434316Z","iopub.status.idle":"2021-10-07T14:40:13.330444Z","shell.execute_reply.started":"2021-10-07T14:37:38.434285Z","shell.execute_reply":"2021-10-07T14:40:13.329182Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"## **Результат**","metadata":{}},{"cell_type":"code","source":"# разделяем выборки обратно на тренировочную и тестовую\ntrain = df.query('train == 1').drop(['train'], axis=1)\ntest = df.query('train == 0').drop(['train'], axis=1)\n\n\nX_train = train.drop('default', axis=1)\ny_train = train['default']\nX_test = test.drop('default', axis=1)\n\n# стандартизируем отдельно\nscaler = StandardScaler().fit(X_train)\nX_train_st = scaler.transform(X_train)\nX_test_st = scaler.transform(X_test)\n\n# обучаем модель, используя подобранные параметры \nm = LogisticRegression(C=100, penalty='l2', solver='newton-cg')\nm.fit(X_train_st, y_train)\ny_pred = m.predict(X_test_st)\ny_pred_proba = m.predict_proba(X_test_st)\ny_pred_proba = y_pred_proba[:,1]\n","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:40:13.332295Z","iopub.execute_input":"2021-10-07T14:40:13.332638Z","iopub.status.idle":"2021-10-07T14:40:16.147557Z","shell.execute_reply.started":"2021-10-07T14:40:13.332602Z","shell.execute_reply":"2021-10-07T14:40:16.146486Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"results_df = pd.DataFrame(data={'client_id': test['client_id'], 'default': y_pred_proba})\nresults_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:40:16.150404Z","iopub.execute_input":"2021-10-07T14:40:16.150914Z","iopub.status.idle":"2021-10-07T14:40:16.311245Z","shell.execute_reply.started":"2021-10-07T14:40:16.150863Z","shell.execute_reply":"2021-10-07T14:40:16.309838Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"results_df","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:40:16.312798Z","iopub.execute_input":"2021-10-07T14:40:16.313179Z","iopub.status.idle":"2021-10-07T14:40:16.330434Z","shell.execute_reply.started":"2021-10-07T14:40:16.313143Z","shell.execute_reply":"2021-10-07T14:40:16.329185Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"## ** Miscellaneous","metadata":{}},{"cell_type":"markdown","source":"* возгалались большие надежды на oversampling. При этом в любом из случаев (как методом SMOTE, так и Random) модель показывала худший результат, поэтому пришлось вовсе отказаться \n* пыталась найти лучший параметр class_weight для логистической регрессии через GridSearch. Так и не удалось заставить код работать из-за convergence error, настройка max_iter при этом не помогла. Подбирала вручную разные рандомные значения, при этом результат ухудшался.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}